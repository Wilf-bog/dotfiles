
<p>One of the more reasonable use cases for modern &#8220;AI&#8221; (statistical pattern matching and generating machines) is to support doctors in diagnostics, especially in the evaluation of complex data sets / documents in order to determine potentially dangerous abnormalities.</p>



<p>It&#8217;s a problem tailor-made for neural networks (it&#8217;s actually one of the types of cases with that we got trained on them back when I was still studying in the beginning of the 2000s): You have large more or less clean and tagged sets of data of diagnosed x-rays and such that you can feed into your system to train a detector. Now since human beings are not really standardized/normalized the results are never 100%, one person&#8217;s abnormal is another person&#8217;s normal. But you can get decent results.</p>



<p>Results that &#8220;AI&#8221; hypesters often celebrate: <em>Look AI&#8217;s not a hype, studies show that AI systems are better than X% of doctors at diagnosing Y!</em></p>



<p>I think that many people don&#8217;t fully understand the context though: What we see time and time again is that these systems are better than a significant percentage of doctors at diagnoses. Cool. But <em>which</em> doctors?</p>



<p>More often than not you will find that these systems are <em>better</em> than new or unexperienced doctors but <em>worse</em> than experienced specialists. Which isn&#8217;t surprising. It&#8217;s like with text: ChatGPT and others generate text that&#8217;s decent-ish and better than what first semester students would write but usually a lot worse than what experienced professionals create. AI is cheap and mediocre.</p>



<p>Now we could say that that&#8217;s a positive: We&#8217;ve basically shifted the baseline up. No longer do you need to hope that a new doctor diagnoses your condition correctly, you have the AI level as baseline and more complex cases get sent to the experts (how many cases would be found as complex when the mediocre machine has already diagnosed the case is another very good question).</p>



<p>But there is an issue (one that you can see coming from a mile if you&#8217;ve every heard one of my talks on &#8220;AI&#8221;): How do young, inexperienced doctors <em>learn</em>, if the machines diagnose all the routine cases? You learn diagnoses by diagnosing, by talking about them with your peers, especially more experienced peers. It&#8217;s great that we&#8217;ve potentially made the baseline better but how do we ensure that young physicians get enough time to practice their diagnostic skills? Because the super experienced people will die at some point. And experience does neither come cheap nor quick.</p>



<p>Especially in a world where the main promise of &#8220;AI&#8221; is efficiency and productivity do we really believe that the &#8220;time savings&#8221; will be put into training young people who can&#8217;t yet do the work? When it takes years to get them to where they need to be to catch the stuff that the AIs didn&#8217;t find? In a different system we might but in neoliberal capitalism? In austerity?</p>



<p>It&#8217;s great that we look at reasonable use cases for statistics, especially in critical and highly complex domains like medicine. But the studies presenting successes need to be read and understood correctly and fully.</p>



<p>Better than X% of humans is an empty statement. You need to know what exactly characterizes the X% to understand the actual capabilities and limitations of the systems that are supposedly so great.</p>
