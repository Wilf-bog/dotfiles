
<p><em>(I wrote this essay for the printed magazine of the <a href="https://elevate.at">Elevate Festival 2024</a>. On Friday March 1st. at 2pm I will participate in a <a href="https://elevate.at/diskurs/programm/e24aivsdemocracy/">panel discussion there on the issue of &#8220;AI vs. Democracy&#8221;</a> that people can check out live or on stream/watch in a recording later [it is in German though])</em></p>



<p>For a long time, we as western societies have looked towards tech when it comes to the future of both civilisation and democracy. The Internet was to bring democracy to every human on this planet (and Mars when people still believed a word of what Elon Musk says &#8211; those were innocent days). Cryptocurrencies would democratise finance, somehow. By now we know the format.</p>



<p>But the current hype of AI behaves slightly differently. Of course there are promises of technologies doing all kinds of magic, but AI as a discursive hyperobject is also wrapped in the stories of doom and danger as a strange form of contradictory advertising Lee Vinsel once called “<a href="https://sts-news.medium.com/youre-doing-it-wrong-notes-on-criticism-and-technology-hype-18b08b4307e5">criti-hype</a>”.</p>



<p>However, because AI is finding its way into everything right now we are of course thinking about our societies and political frameworks. What will AI do to or for democracy?&nbsp;&nbsp;</p>



<h3 class="wp-block-heading">Misinformation and Radicalization</h3>



<p>When talking about AI and democracy, misinformation is the current go-to narrative. Dishonest actors can use generative AI systems to create fake material about their political opponents from mere photos to audio or so-called deep fake videos.But while this narrative sounds plausible, one has to wonder if it’s really happening. Are people really that easy to sway in their political beliefs, is the hypodermic needle theory of communication true? Can mere exposure to an idea plant that idea into people’s minds?</p>



<p><a href="https://misinforeview.hks.harvard.edu/article/misinformation-reloaded-fears-about-the-impact-of-generative-ai-on-misinformation-are-overblown/">Current studies</a> have shown the <em>fear of misinformation to be massively overblown</em>, with both people’s political leanings and media literacy proving significantly stronger. What can be seen is that right-wing actors in particular are using generative AI systems to further radicalise their existing target audience.</p>



<p>In this case it’s not so much about creating a perfect fake &#8211; <em>truth</em> is not really relevant in that scene &#8211; but about enabling more people to create material to engage their base quicker and easier. It’s used as <em>an automation tool to keep their fires of outrage burning</em>. We’ve seen such activity for example within the US Q-Anon movement. The massive amount of material that AI allows the creation of makes it harder and harder to reach people inside with true information.&nbsp;</p>



<h3 class="wp-block-heading">Centralization of Power</h3>



<p>While AI is software, it’s a bit atypical in its requirements. Running bigger AI models requires massive amounts of expensive, specialised computer hardware, and a lot of electrical power and cooling. Building one’s own models also requires lots and lots of data that needs to be gathered, preprocessed and stored. Because of this required scale, modern AI systems have a built-in tendency towards centralisation. You can indeed pick whether to run your models with Microsoft, Amazon, Google or Baidu –&nbsp;but you are basically forced to pick one from this oligopoly.</p>



<p>With the growing push to integrate these systems into applications and processes throughout society, from administration to healthcare to journalism to everything else, society&#8217;s dependence on the infrastructures of Big Tech is only going to increase.But it’s not just about the hardware infrastructures that these so-called hyperscalers can provide, it’s also about the required <em>data</em>. Amazon, Microsoft and Google sit on large pools of user-generated data they can use to train and tweak their models, a lead that will be hard to catch up on by any possible competition.&nbsp;</p>



<p>The opacity of these models &#8211; meaning the lack of explainability and clarity making it very hard to understand <em>why </em>an AI system produced what it produced &#8211; turns this economic centralisation into even more of a power centralisation. What do our democratic and cultural values mean when the systems that are used to inform (if not even make) decisions have been trained on data based on a fully different understanding of the world?</p>



<h3 class="wp-block-heading">Epistemic Injustice</h3>



<p>AI is supposed to make us all more productive, even starting in school. Why learn essay writing when an AI can generate you one you just have to fact check? Just learn “prompting” to get what you need. “Prompting” (as in writing the phrases to use to trigger an AI system) is indistinguishable from spellcasting, AI systems communicate a form of magic thinking about the world where you get anything you need if you just cast the right spell.</p>



<p>Writing, however, like drawing, painting, coding or illustration, may not be magic, but remains a form of thinking essential to the learning process; to developing both aesthetic and cognitive sensibilities through practice.</p>



<p>AIs can therefore <a href="https://tante.cc/2024/01/03/is-a-neural-network-like-a-pocket-calculator-ai-and-epistemic-injustice/">create a form of epistemic injustice</a> &#8211; meaning a form of injustice in the acquisition and communication of knowledge &#8211; destroying the spaces in which people learn and grow. This issue gets more serious when realising that AI systems are always trained on data from the past. AI systems can confidently reproduce every style in which a human has ever written or painted. But they can’t create the <em>new</em>, they are always a rehashing of the past (a danger Timnit Gebru, Emiliy Bender, Angelina McMillan-Major and Margaret Mitchell already talked about in their paper “<a href="https://dl.acm.org/doi/10.1145/3442188.3445922">On the Dangers of Stochastic Parrots</a>” in 2021).</p>



<p>Democracy requires informed citizens, people being able to form an understanding of the world and challenge the status quo to move society forward. AIs <em>can </em>(though not necessarily <em>must</em>) take away this ability people have to fully make sense of the world and their situation in it. They can elicit a form of infantilization that leaves people forced to increasingly “trust the AI” &#8211; AI’s that, as we already talked about, are run by for-profit corporations from the US (or China).</p>



<h3 class="wp-block-heading">“The cuts will increase until morale improves…”</h3>



<p>When we hear stories about AI applications the sales pitch is more often than not productivity/optimisation. Which mostly means firing people or not hiring them.&nbsp;</p>



<p>AI is supposed to do care work in hospitals and outside (through apps or robots) because paying actual care workers a fair wage “is too expensive”. Insurances and even government institutions get rid of the people doing case work and replace them with AI because it’s cheaper. Good luck appealing to the ChatBot’s humanity when you don’t get the service or support you need.</p>



<p>We know this narrative as <em>Neoliberalism</em>: everything is a business and expenses have to be cut until they can’t be cut anymore. The way AIs are framed is only putting rocket fuel on this fire because &#8211; currently &#8211; AI can promise anything. In just a few months AI will be able to do <em>anything</em> (since 1960 AIs capable of doing anything a human can do better has always been 10 years away), so why invest in our infrastructures and systems? AI is gonna fix it. Why bother doing the hard and painful work to fight the climate crisis when you can just claim an AI is gonna fix it soon<img src="https://s.w.org/images/core/emoji/15.0.3/72x72/2122.png" alt="™" class="wp-smiley" style="height: 1em; max-height: 1em;" />?</p>



<p>In this reading AI is taking away freedoms and democratic choice. By framing political questions on rights of people and resource distribution as equations to be somehow solved “optimally”&nbsp;– rather than political struggles of different interests in the space for debate –&nbsp;the reduced ability to challenge even a narrative as mainstream as neoliberal capitalism inherent in AI systems impoverishes democracies. And whose interests will these systems running on the machines owned by the richest and most powerful people on the planet serve? Poor workers’? Guess again.</p>



<h3 class="wp-block-heading">Conclusion</h3>



<p>The discourse on AI and democracy is often dominated by superficial worries infused by a deeply rooted bias of intellectuals against the general public. Misinformation will make the masses vote for “the wrong people”.</p>



<p>While there are issues with using these systems for radicalisation, the more pressing issues of AI are about power and about how people can learn about and reflect on their position in the world, about how people can fight for their rights and demands.&nbsp;</p>



<p>In the way they are framed and deployed today, AIs close doors and lay havoc to the intellectual spaces and practices that allow our democracies to flourish. The fight about AI is a political battle about the rights of the individual and the social values of our societies – and not about technology.</p>
